{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\n!{sys.executable} -m pip install matplotlib","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow_datasets as tfds\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-12-21T23:50:50.031054Z","iopub.execute_input":"2021-12-21T23:50:50.031943Z","iopub.status.idle":"2021-12-21T23:50:55.422220Z","shell.execute_reply.started":"2021-12-21T23:50:50.031826Z","shell.execute_reply":"2021-12-21T23:50:55.421325Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T23:50:55.428155Z","iopub.execute_input":"2021-12-21T23:50:55.430614Z","iopub.status.idle":"2021-12-21T23:51:52.100349Z","shell.execute_reply.started":"2021-12-21T23:50:55.430553Z","shell.execute_reply":"2021-12-21T23:51:52.099615Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2021-12-21 23:50:55.545730: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Not found: Could not locate the credentials file.\". Retrieving token from GCE failed with \"Failed precondition: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6570e75974e40128f0fc9ace5c5f71e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8402c7f9b6449eab5ce84c92d89aa26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling imdb_reviews-train.tfrecord...:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling imdb_reviews-test.tfrecord...:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling imdb_reviews-unsupervised.tfrecord...:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"2021-12-21 23:51:49.799131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-21 23:51:49.884151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-21 23:51:49.884862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-21 23:51:49.886978: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2021-12-21 23:51:49.887279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-21 23:51:49.887959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-21 23:51:49.888557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-21 23:51:51.678752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-21 23:51:51.679522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-21 23:51:51.680144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-21 23:51:51.680736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset, test_dataset = dataset['train'], dataset['test']\n\ntrain_dataset.element_spec","metadata":{"execution":{"iopub.status.busy":"2021-12-21T23:51:52.101455Z","iopub.execute_input":"2021-12-21T23:51:52.101695Z","iopub.status.idle":"2021-12-21T23:51:52.108890Z","shell.execute_reply.started":"2021-12-21T23:51:52.101655Z","shell.execute_reply":"2021-12-21T23:51:52.108103Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(TensorSpec(shape=(), dtype=tf.string, name=None),\n TensorSpec(shape=(), dtype=tf.int64, name=None))"},"metadata":{}}]},{"cell_type":"code","source":"for example, label in train_dataset.take(1):\n  print('text: ', example.numpy())\n  print('label: ', label.numpy())","metadata":{"execution":{"iopub.status.busy":"2021-12-21T23:51:52.110776Z","iopub.execute_input":"2021-12-21T23:51:52.111305Z","iopub.status.idle":"2021-12-21T23:51:52.204033Z","shell.execute_reply.started":"2021-12-21T23:51:52.111268Z","shell.execute_reply":"2021-12-21T23:51:52.203348Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2021-12-21 23:51:52.150279: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\nlabel:  0\n","output_type":"stream"},{"name":"stderr","text":"2021-12-21 23:51:52.196565: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}]},{"cell_type":"code","source":"VOCAB_SIZE = 5000\nencoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\nencoder.adapt(train_dataset.map(lambda text, label: text))","metadata":{"execution":{"iopub.status.busy":"2021-12-21T23:52:05.629207Z","iopub.execute_input":"2021-12-21T23:52:05.629478Z","iopub.status.idle":"2021-12-21T23:52:44.772490Z","shell.execute_reply.started":"2021-12-21T23:52:05.629447Z","shell.execute_reply":"2021-12-21T23:52:44.771651Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"BUFFER_SIZE = 10000\nBATCH_SIZE = 64","metadata":{"execution":{"iopub.status.busy":"2021-12-21T23:52:44.774587Z","iopub.execute_input":"2021-12-21T23:52:44.774880Z","iopub.status.idle":"2021-12-21T23:52:44.779276Z","shell.execute_reply.started":"2021-12-21T23:52:44.774840Z","shell.execute_reply":"2021-12-21T23:52:44.778608Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T23:52:44.780440Z","iopub.execute_input":"2021-12-21T23:52:44.781233Z","iopub.status.idle":"2021-12-21T23:52:44.794117Z","shell.execute_reply.started":"2021-12-21T23:52:44.781185Z","shell.execute_reply":"2021-12-21T23:52:44.793300Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])","metadata":{"execution":{"iopub.status.busy":"2021-12-21T23:52:44.797285Z","iopub.execute_input":"2021-12-21T23:52:44.797889Z","iopub.status.idle":"2021-12-21T23:52:44.849308Z","shell.execute_reply.started":"2021-12-21T23:52:44.797859Z","shell.execute_reply":"2021-12-21T23:52:44.848684Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-12-21T23:52:44.850313Z","iopub.execute_input":"2021-12-21T23:52:44.850565Z","iopub.status.idle":"2021-12-21T23:52:44.866516Z","shell.execute_reply.started":"2021-12-21T23:52:44.850529Z","shell.execute_reply":"2021-12-21T23:52:44.865829Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset, epochs=5,\n                    validation_data=test_dataset,\n                    validation_steps=30)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T23:52:44.867718Z","iopub.execute_input":"2021-12-21T23:52:44.868142Z","iopub.status.idle":"2021-12-21T23:59:02.982206Z","shell.execute_reply.started":"2021-12-21T23:52:44.868104Z","shell.execute_reply":"2021-12-21T23:59:02.980859Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"2021-12-21 23:52:59.712160: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"391/391 [==============================] - 77s 151ms/step - loss: 0.5797 - accuracy: 0.6329 - val_loss: 0.3742 - val_accuracy: 0.8307\n","output_type":"stream"},{"name":"stderr","text":"2021-12-21 23:54:01.440345: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5\n391/391 [==============================] - 56s 141ms/step - loss: 0.3088 - accuracy: 0.8729 - val_loss: 0.3095 - val_accuracy: 0.8609\n","output_type":"stream"},{"name":"stderr","text":"2021-12-21 23:54:56.980475: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5\n391/391 [==============================] - 56s 143ms/step - loss: 0.2417 - accuracy: 0.9062 - val_loss: 0.3329 - val_accuracy: 0.8635\n","output_type":"stream"},{"name":"stderr","text":"2021-12-21 23:56:19.371692: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5\n391/391 [==============================] - 56s 142ms/step - loss: 0.2109 - accuracy: 0.9206 - val_loss: 0.3351 - val_accuracy: 0.8771\n","output_type":"stream"},{"name":"stderr","text":"2021-12-21 23:57:41.083990: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5\n391/391 [==============================] - 56s 142ms/step - loss: 0.1903 - accuracy: 0.9286 - val_loss: 0.3269 - val_accuracy: 0.8547\n","output_type":"stream"},{"name":"stderr","text":"2021-12-21 23:59:02.970721: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}]}]}